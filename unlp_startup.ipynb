{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZQKX1UAEKw5"
   },
   "source": [
    "## BERT&co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeGAu8raEEIG"
   },
   "source": [
    "ML-часть, распиленная на пайплайны:\n",
    "\n",
    "    Обучатор берта. Возьмём версию на PyTorch от huggingface. Для английского отсутствует — можно взять предобученный от гугла.\n",
    "    Первый дообучатор берта. Обучается на вопросах-ответах как болталка. После этого ответная башня выкидывается и сохранаятся только вопросная — болталка нам не нужна.\n",
    "    Второй дообучатор берта. Обучается ранжировать (если данных совсем много — классифицировать) только вопросы по близости через триплет лосс. Требует много реальной разметки, не обязателен.\n",
    "    Парсер диалоговых данных. Сначала возьмём какой-нибудь ubuntu dialogue corpus, но в будущем нужно будет напарсить какой-нибудь твиттер или реддит и хорошо дообучиться на них.\n",
    "\n",
    "В репозитории ml должны быть скрипты для сбора данных (изначально только wget убунту диалог корпуса) и пайплайн для дообучения берта под диалоги. That's it. Результатом основного скрипта для обучения будут два файла — сериализованная моделька и токенизатор — и, возможно, какие-нибудь скрипты, чтобы их можно было использовать бэкэнду на чистом сервере.\n",
    "\n",
    "За основу имеет смысл взять тот репозиторий от huggingface. ЕМНИП, там токенизатор встроен в модель или куда-то на высоком уровне.\n",
    "\n",
    "Там можно несложными хаками докрутить поверх эмбеддера ещё голову, которая будет делать ранжирование (нужно два раза инициализировать берт — сиамская сеть же, нужны две разные башни). Само обучение будет выглядеть так: нарезать данные формата вопрос-правильный_ответ и засунуть в большой батч (скажем, 64 примера), внутри которого для каждого вопроса все остальные 63 ответа считаются негативными. Векторизовав весь батч и посчитав «матрицу умножения», то есть все попарные скалярные произведения, можно эффективнее считать какой-нибудь лосс для ранжирования (см. презентацию).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "kw1ft9QPEPEq",
    "outputId": "e7fd6a36-abe0-4af4-9e24-b10d0bb3c77d"
   },
   "outputs": [],
   "source": [
    "#!pip3 install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzP_Er1nD-U_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6pylSzmEayc"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LEa1ahtMDur"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "u6bBNFZVEosT",
    "outputId": "c7d9e183-5150-401d-d269-2946b9b7ceae"
   },
   "outputs": [],
   "source": [
    "!./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLTMcV3uJq44"
   },
   "source": [
    "Корпус влезет в оперативную память. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZYPxNIBbWjPV",
    "outputId": "96fe851b-8da1-46b8-f83f-8ccd2af5ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test [link] lets see this too [link] end\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '[link]', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "\n",
    "print( remove_urls(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf?233/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "ok8peEdaN-SW",
    "outputId": "d9a294b3-721a-4388-951e-60207b39bb84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./pretrained-bert-base-uncased/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') #('cpu')\n",
    "bert_type = 'bert-base-uncased'\n",
    "max_seq_len = 512 # BERT-BASE restriction\n",
    "cache_dir = './pretrained-' + bert_type\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0TaLgKywJyBn",
    "outputId": "f059e65f-357e-4dfb-b006-4ba73d63f95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "class UbuntuCorpus(Dataset):\n",
    "  def __init__(self, tokenizer, rootdir='./dialogs'):\n",
    "    super(UbuntuCorpus, self).__init__()\n",
    "    dialogs = []\n",
    "    _cnt = 3000 # debug constant\n",
    "    \n",
    "    # punctuations signs after which we put [SEP] token\n",
    "    punctuation_seps = ['?!', '!?', '?', '...', '. '] \n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for subdir in os.listdir(rootdir):\n",
    "      for dialog in os.listdir(rootdir + '/' + subdir):\n",
    "        path = rootdir + '/' + subdir +'/' + dialog \n",
    "        with open(path) as tsvfile:\n",
    "          reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "          rows = [(row[1], row[-1]) for row in reader]\n",
    "          replicas = []\n",
    "          authors = set()\n",
    "          author = -1\n",
    "          for row in rows:\n",
    "            if author == row[0]:\n",
    "              replicas[-1].append(row[1])\n",
    "            else:\n",
    "              author = row[0]\n",
    "              authors.add(author)\n",
    "              replicas.append([row[1]])\n",
    "              \n",
    "          '''\n",
    "          Answer replic is a replic without ?\n",
    "          Question replic is a replic with ? followed by answer replic\n",
    "          \n",
    "          Both must be longer than thr (after link replacemenets)\n",
    "          \n",
    "          And due to BERT restrictions in tokenized form shorter than max_seq_len\n",
    "          '''\n",
    "          \n",
    "          for i in range(len(replicas)):\n",
    "            replicas[i] = '[CLS] ' + remove_urls(' '.join(replicas[i]))\n",
    "            \n",
    "            codephrase = 'evilcyborgswillkillhumanity'\n",
    "            \n",
    "            sep_token = '[SEP]'\n",
    "            \n",
    "            for (ind, el) in enumerate(punctuation_seps):\n",
    "              crouch = codephrase + f'{ind} ' + sep_token + ' '\n",
    "              replicas[i] = replicas[i].replace(el, crouch)\n",
    "            \n",
    "            for (ind, el) in enumerate(punctuation_seps):\n",
    "              replicas[i] = replicas[i].replace(codephrase + f'{ind}', el)\n",
    "              \n",
    "            \n",
    "            if replicas[i].rstrip()[-len(sep_token):] != sep_token:\n",
    "              replicas[i] = replicas[i] + ' ' + sep_token\n",
    "          \n",
    "          thr = 120\n",
    "          \n",
    "          for i in range(len(replicas) - 1):\n",
    "            if replicas[i].count('?') > 0 and replicas[i + 1].count('?') == 0 \\\n",
    "              and min(len(replicas[i]), len(replicas[i + 1])) > thr \\\n",
    "              and len(tokenizer.tokenize(replicas[i])) <= max_seq_len \\\n",
    "              and len(tokenizer.tokenize(replicas[i + 1])) <= max_seq_len:\n",
    "              qa_pairs.append([replicas[i], replicas[i + 1]])\n",
    "              _cnt -= 1\n",
    "              if _cnt <=0:\n",
    "                break\n",
    "          \n",
    "          \n",
    "    \n",
    "          #for replica in replicas:\n",
    "          #  print('>>>', replica)\n",
    "          #  print()\n",
    "          #print(authors)\n",
    "          #print()\n",
    "          #print()\n",
    "        \n",
    "        if _cnt <= 0:\n",
    "          break\n",
    "      if _cnt <=0:\n",
    "          break\n",
    "    '''for el in qa_pairs:\n",
    "      print('>>', el[0])\n",
    "      print('>>>', el[1])\n",
    "      print()'''\n",
    "    \n",
    "    self.qa_pairs = qa_pairs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.qa_pairs)\n",
    "  \n",
    "  def __getitem__(self, ind):\n",
    "    '''\n",
    "    #self.tokenizer = ??\n",
    "    answ = [self.qa_pairs[ind][1]]\n",
    "    used = {ind}\n",
    "    while len(answ) != batch_size:\n",
    "      ind2 = random.randint(0, len(self) - 1)     \n",
    "      if ind2 not in used:\n",
    "        used.add(ind2)\n",
    "        answ.append(self.qa_pairs[ind2][1])\n",
    "    #print('#%^^&', answ)\n",
    "    '''\n",
    "    return (self.qa_pairs[ind][0], self.qa_pairs[ind][1])#answ)      \n",
    "        \n",
    "corpus = UbuntuCorpus(tokenizer) # full corpus, 1,917,802 qa pairs \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ru79A8XiQxtG",
    "outputId": "c0ca9447-59e3-4e3b-c3a9-e89df057f4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 15 19:50:48 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:06:00.0  On |                  N/A |\r\n",
      "| 27%   45C    P8    22W / 175W |    357MiB /  7949MiB |     11%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1170      G   /usr/lib/xorg/Xorg                            18MiB |\r\n",
      "|    0      1211      G   /usr/bin/gnome-shell                          50MiB |\r\n",
      "|    0      1539      G   /usr/lib/xorg/Xorg                           146MiB |\r\n",
      "|    0      1790      G   /usr/bin/gnome-shell                         134MiB |\r\n",
      "|    0      2754      G   /usr/lib/firefox/firefox                       3MiB |\r\n",
      "|    0      7092      G   gnome-control-center                           3MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeSOc5PAcZzO"
   },
   "outputs": [],
   "source": [
    "#pickle.dump(corpus, open( \"./corpus.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "UTMFxa1Ndslr",
    "outputId": "6d8abb1f-750e-46cb-f505-de6dbfc0f853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I am attempting to install GnuGo & Quarry (a GUI to handle GnuGo); I was going to install the tarball, and had downloaded the tars for both, but then I found them both on aptitude.  [SEP] So I installed them (I think? [SEP] ) with aptitude's terminal UI.  [SEP] How do I use them now? [SEP]  typing in \"quarry\" gives me nothing.  [SEP] I am attempting to install GnuGo & Quarry (a GUI to handle GnuGo); I was going to install the tarball, and had downloaded the tars for both, but then I found them both on aptitude.  [SEP] So I installed them (I think? [SEP] ) with aptitude's terminal UI.  [SEP] How do I use them now? [SEP]  Typing in \"quarry\" gives me nothing.  [SEP] Can anyone help me understand Aptitude? [SEP]  kubuntu? [SEP]  I'm not using KDE, I use fluxbox.  [SEP] Mark, I'm using terminal.  [SEP] I've never used the gui, don't particularly want to.  [SEP] mark ryan: I just used sudo aptitude, found two programs I wanted (gnugo & quarry), downloaded them (or so it seems? [SEP] ) and now I don't know what to do.  [SEP] Does aptitude actually install the program, or just download the tar and all dependencies? [SEP] \n",
      "['[CLS]', 'i', 'am', 'attempting', 'to', 'install', 'gnu', '##go', '&', 'quarry', '(', 'a', 'gui', 'to', 'handle', 'gnu', '##go', ')', ';', 'i', 'was', 'going', 'to', 'install', 'the', 'tar', '##ball', ',', 'and', 'had', 'downloaded', 'the', 'tar', '##s', 'for', 'both', ',', 'but', 'then', 'i', 'found', 'them', 'both', 'on', 'apt', '##itude', '.', '[SEP]', 'so', 'i', 'installed', 'them', '(', 'i', 'think', '?', '[SEP]', ')', 'with', 'apt', '##itude', \"'\", 's', 'terminal', 'ui', '.', '[SEP]', 'how', 'do', 'i', 'use', 'them', 'now', '?', '[SEP]', 'typing', 'in', '\"', 'quarry', '\"', 'gives', 'me', 'nothing', '.', '[SEP]', 'i', 'am', 'attempting', 'to', 'install', 'gnu', '##go', '&', 'quarry', '(', 'a', 'gui', 'to', 'handle', 'gnu', '##go', ')', ';', 'i', 'was', 'going', 'to', 'install', 'the', 'tar', '##ball', ',', 'and', 'had', 'downloaded', 'the', 'tar', '##s', 'for', 'both', ',', 'but', 'then', 'i', 'found', 'them', 'both', 'on', 'apt', '##itude', '.', '[SEP]', 'so', 'i', 'installed', 'them', '(', 'i', 'think', '?', '[SEP]', ')', 'with', 'apt', '##itude', \"'\", 's', 'terminal', 'ui', '.', '[SEP]', 'how', 'do', 'i', 'use', 'them', 'now', '?', '[SEP]', 'typing', 'in', '\"', 'quarry', '\"', 'gives', 'me', 'nothing', '.', '[SEP]', 'can', 'anyone', 'help', 'me', 'understand', 'apt', '##itude', '?', '[SEP]', 'ku', '##bu', '##nt', '##u', '?', '[SEP]', 'i', \"'\", 'm', 'not', 'using', 'k', '##de', ',', 'i', 'use', 'flux', '##box', '.', '[SEP]', 'mark', ',', 'i', \"'\", 'm', 'using', 'terminal', '.', '[SEP]', 'i', \"'\", 've', 'never', 'used', 'the', 'gui', ',', 'don', \"'\", 't', 'particularly', 'want', 'to', '.', '[SEP]', 'mark', 'ryan', ':', 'i', 'just', 'used', 'sud', '##o', 'apt', '##itude', ',', 'found', 'two', 'programs', 'i', 'wanted', '(', 'gnu', '##go', '&', 'quarry', ')', ',', 'downloaded', 'them', '(', 'or', 'so', 'it', 'seems', '?', '[SEP]', ')', 'and', 'now', 'i', 'don', \"'\", 't', 'know', 'what', 'to', 'do', '.', '[SEP]', 'does', 'apt', '##itude', 'actually', 'install', 'the', 'program', ',', 'or', 'just', 'download', 'the', 'tar', 'and', 'all', 'depend', '##encies', '?', '[SEP]']\n",
      "----------------\n",
      "[CLS] it installs it.  [SEP]  It odens't use tars though, it uses debs try dpkg -L quarry | grep /usr/bin then you haven't installed it try sudo aptitude install quarry Either start with a CLI version from the alternate CD, or if you want to remove everything Ubuntu installed, follow these steps: [link]  only don't do the install of xubuntu desktop (at the end of that line) [SEP]\n"
     ]
    }
   ],
   "source": [
    "item = corpus[random.randint(0, len(corpus) - 1)]\n",
    "#print(item)\n",
    "print(item[0])\n",
    "print(tokenizer.tokenize(item[0]))\n",
    "print('----------------')\n",
    "\n",
    "print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fkK2SkiMzeJL",
    "outputId": "9773d9ce-1bf1-49c2-8bb8-2794fd90b6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"[CLS] what version of ubuntu is this ? [SEP]  right click on the pannel and try to re-add gnome-network manager or launch it manually from the command line, see if it shows you why it's crashing [SEP]\", '[CLS] howso? [SEP]  just before you said it does matter, i figured id put the cd i used to install 10.04 to begin with in, and tr it [SEP]'), ('[CLS] it start without problem but in wired connection and wireless network not appear nothing about adapters.  [SEP] they are disabled (but working at least eth0) [SEP]', \"[CLS] If it's 10.10 Desktop that makes it easier because 10.10 has a loopback.cfg: [link] .  [SEP] If it's an Alternate install iso then you'll have problems with any version number of Ubuntu. [SEP]\")]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "batch = None\n",
    "for el in trainloader:\n",
    "  batch = el\n",
    "  break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nww6TrrdiUmD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_batch(batch):\n",
    "  (quests, answs) = batch\n",
    "  quests = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in quests]\n",
    "  answs = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in answs]\n",
    "  \n",
    "  quest_segments = [torch.tensor([[0 for i in range(len(quests[j]))]]) for j in range(len(quests))]\n",
    "  answ_segments = [torch.tensor([[0 for i in range(len(answs[j] ))]]) for j in range(len(answs))]\n",
    "  \n",
    "  quests = [torch.tensor([el]) for el in quests]\n",
    "  answs = [torch.tensor([el]) for el in answs]\n",
    "  \n",
    "  return ((quests, quest_segments), (answs, answ_segments))\n",
    "  \n",
    "prepare_batch(batch)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ojqkz_tswXP-",
    "outputId": "6417c77d-9a4e-48f3-b1e1-09e7260a7a3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embeddings):\n",
    "  '''\n",
    "  using default bert-as-service strategy to get fixed-size vector\n",
    "  1. considering only -2 layer\n",
    "  2. \"REDUCE_MEAN \ttake the average of the hidden state of encoding layer on the time axis\" @bert-as-service  \n",
    "  '''\n",
    "  embeddings = embeddings[-2]\n",
    "  result = torch.sum(embeddings, dim=1)\n",
    "  \n",
    "  return result.to(device)\n",
    "\n",
    "def embed_batch(batch, qembedder, aembedder):\n",
    "  ((quests, quest_segments), (answs, answ_segments)) = batch\n",
    "  \n",
    "  #print(quests[0])\n",
    "  #print(quest_segments[0])\n",
    "  \n",
    "  tmp_quest = [get_embedding(qembedder(quests[i].to(device), quest_segments[i].to(device))[0]) for i in range(len(quests))]\n",
    "  tmp_answ = [get_embedding(aembedder(answs[i].to(device), answ_segments[i].to(device))[0]) for i in range(len(answs))]\n",
    "  \n",
    "  qembeddings = torch.cat(tmp_quest)\n",
    "  aembeddings = torch.cat(tmp_answ)\n",
    "    \n",
    "  return (qembeddings, aembeddings)\n",
    "\n",
    "#embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hD6cPbpDW8nR"
   },
   "outputs": [],
   "source": [
    "def hinge_loss(X, Y, margin=0.1):\n",
    "  batch_size = X.shape[0]\n",
    "  similarities = cosine_similarity_table(X, Y)\n",
    "  #^ см. ниже\n",
    "  \n",
    "  identity = torch.eye(batch_size, device=X.device)\n",
    "  non_diagonal = torch.ones_like(similarities) - identity\n",
    "  \n",
    "  targets = identity - non_diagonal\n",
    "  weights = identity + non_diagonal / (batch_size - 1)\n",
    "  \n",
    "  #всё то же самое, но лосс другой: учитываем только то, что не превосходит margin\n",
    "  losses = torch.pow(F.relu(margin - targets * similarities), 2)\n",
    "  return torch.mean(losses * weights)\n",
    "\n",
    "def cosine_similarity_table(X, Y):\n",
    "  X = F.normalize(X)\n",
    "  Y = F.normalize(Y)\n",
    "  return torch.mm(X, Y.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a42F6Nb-yEMc"
   },
   "outputs": [],
   "source": [
    "def bce_loss(X, Y, conf_true=0.9, conf_false=0.1): \n",
    "  '''на вход пришел батч размера n,\n",
    "  мы векторизовали контексты (X)\n",
    "  и ответы (Y) и хотим сделать n*n\n",
    "  независимых классификаций\n",
    "  '''\n",
    "  n = X.shape[0]\n",
    "\n",
    "  logits = torch.mm(X, Y.transpose(0, 1)) # считаем таблицу умножения\n",
    "  identity = torch.eye(n, device=X.device)\n",
    "  \n",
    "  non_diagonal = torch.ones_like(logits) - identity\n",
    "  targets = identity * conf_true + non_diagonal * conf_false\n",
    "  #получаем матрицу с conf_true на диагонали и conf_false где-либо ещё\n",
    "  \n",
    "  weights = identity + non_diagonal / (n - 1)\n",
    "  # ^ чтобы не было перекоса в сторону негативов\n",
    "  return F.binary_cross_entropy_with_logits(logits, targets, weights) * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZP70JppTRF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, dtype=torch.uint8)\n",
      "tensor(0, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def calc_acc(X, Y):\n",
    "    '''на вход пришел батч размера n,\n",
    "    мы векторизовали контексты (X)\n",
    "    и ответы (Y)'''\n",
    "    \n",
    "    csim = cosine_similarity_table(X, Y)\n",
    "    confidence, predictions = csim.max(-1)\n",
    "    avg = confidence.mean().item()\n",
    "    predictions = list(predictions.cpu())\n",
    "    right = 0\n",
    "    for i in range(len(predictions)):\n",
    "        right += predictions[i] == i\n",
    "    return right\n",
    "    \n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))\n",
    "\n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[.1, 0], [0, .1]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YiTAp29kXV6-",
    "outputId": "05069111-7709-42a9-af7f-13377c234779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_optimizer_params(model):\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "  \n",
    "  optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "  ]\n",
    "  \n",
    "  return optimizer_grouped_parameters\n",
    "\n",
    "#get_optimizer_params(qembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(corpus) * .33)\n",
    "train_size = len(corpus) - test_size\n",
    "train_corpus, test_corpus = torch.utils.data.random_split(corpus, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  batch_size = 15\n",
    "  trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "  num_train_optimization_steps = len(corpus) * epochs\n",
    "  \n",
    "  '''\n",
    "  optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=5e-5,\n",
    "                                 warmup=0.1,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  lr = 5e-5\n",
    "  warmup = 0.1\n",
    "  \n",
    "  qoptim = BertAdam(get_optimizer_params(qembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_total=num_train_optimization_steps)\n",
    "  aoptim = BertAdam(get_optimizer_params(aembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_toal=num_train_optimization_steps)\n",
    "  criterion = hinge_loss\n",
    "  \n",
    "  total = right = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in trainloader:\n",
    "        total += len(batch[0])\n",
    "        embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "        right += calc_acc(*embeddings) \n",
    "\n",
    "  qembedder.train()\n",
    "  aembedder.train()\n",
    "  \n",
    "  logger.info(\"***** Running training *****\")\n",
    "  logger.info(\"  Num steps = %d\", num_train_optimization_steps)  \n",
    "  logger.info(f\" right: {right} of {total}\")\n",
    "  \n",
    "  start_training = time.time()\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    start_epoch = time.time()\n",
    "    qembedder.train()\n",
    "    aembedder.train()\n",
    "    for bidx, batch in enumerate(trainloader):\n",
    "      qoptim.zero_grad()\n",
    "      aoptim.zero_grad()\n",
    "      print('batch_index', bidx)\n",
    "      embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "      loss = bce_loss(*embeddings)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "\n",
    "      qoptim.step()\n",
    "      aoptim.step()\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    \n",
    "    total = right = 0\n",
    "    qembedder.eval()\n",
    "    aembedder.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in trainloader:\n",
    "            total += len(batch[0])\n",
    "            embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "            right += calc_acc(*embeddings) \n",
    "\n",
    "    logger.info(f'epoch {epoch} loss: {total_loss} time: {int(end_epoch - start_epoch)}')\n",
    "    logger.info(f\" right: {right} of {total}\")\n",
    "    \n",
    "  end_training = time.time()\n",
    "  logger.info(f'Training is compleated time: {int(end_training - start_training)}')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "4lPnOyLCLuj_",
    "outputId": "96b3b23e-afce-45e7-efaf-f84c3e5a5fb1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpfl9w6j_n\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpzyisnexg\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num steps = 30000\n",
      "INFO:__main__: right: 74 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 0 loss: 1347817.1661987305 time: 241\n",
      "INFO:__main__: right: 204 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 1 loss: 130649.66033935547 time: 238\n",
      "INFO:__main__: right: 205 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 2 loss: 111859.43228149414 time: 237\n",
      "INFO:__main__: right: 201 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 3 loss: 93859.02690124512 time: 235\n",
      "INFO:__main__: right: 197 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 4 loss: 94527.99389648438 time: 233\n",
      "INFO:__main__: right: 204 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n",
      "batch_index 28\n",
      "batch_index 29\n",
      "batch_index 30\n",
      "batch_index 31\n",
      "batch_index 32\n",
      "batch_index 33\n",
      "batch_index 34\n",
      "batch_index 35\n",
      "batch_index 36\n",
      "batch_index 37\n",
      "batch_index 38\n",
      "batch_index 39\n",
      "batch_index 40\n",
      "batch_index 41\n",
      "batch_index 42\n",
      "batch_index 43\n",
      "batch_index 44\n",
      "batch_index 45\n",
      "batch_index 46\n",
      "batch_index 47\n",
      "batch_index 48\n",
      "batch_index 49\n",
      "batch_index 50\n",
      "batch_index 51\n",
      "batch_index 52\n",
      "batch_index 53\n",
      "batch_index 54\n",
      "batch_index 55\n",
      "batch_index 56\n",
      "batch_index 57\n",
      "batch_index 58\n",
      "batch_index 59\n",
      "batch_index 60\n",
      "batch_index 61\n",
      "batch_index 62\n",
      "batch_index 63\n",
      "batch_index 64\n",
      "batch_index 65\n",
      "batch_index 66\n",
      "batch_index 67\n",
      "batch_index 68\n",
      "batch_index 69\n",
      "batch_index 70\n",
      "batch_index 71\n",
      "batch_index 72\n",
      "batch_index 73\n",
      "batch_index 74\n",
      "batch_index 75\n",
      "batch_index 76\n",
      "batch_index 77\n",
      "batch_index 78\n",
      "batch_index 79\n",
      "batch_index 80\n",
      "batch_index 81\n",
      "batch_index 82\n",
      "batch_index 83\n",
      "batch_index 84\n",
      "batch_index 85\n",
      "batch_index 86\n",
      "batch_index 87\n",
      "batch_index 88\n",
      "batch_index 89\n",
      "batch_index 90\n",
      "batch_index 91\n",
      "batch_index 92\n",
      "batch_index 93\n",
      "batch_index 94\n",
      "batch_index 95\n",
      "batch_index 96\n",
      "batch_index 97\n",
      "batch_index 98\n",
      "batch_index 99\n",
      "batch_index 100\n",
      "batch_index 101\n",
      "batch_index 102\n",
      "batch_index 103\n",
      "batch_index 104\n",
      "batch_index 105\n",
      "batch_index 106\n",
      "batch_index 107\n",
      "batch_index 108\n",
      "batch_index 109\n",
      "batch_index 110\n",
      "batch_index 111\n",
      "batch_index 112\n",
      "batch_index 113\n",
      "batch_index 114\n",
      "batch_index 115\n",
      "batch_index 116\n",
      "batch_index 117\n",
      "batch_index 118\n",
      "batch_index 119\n",
      "batch_index 120\n",
      "batch_index 121\n",
      "batch_index 122\n",
      "batch_index 123\n",
      "batch_index 124\n",
      "batch_index 125\n",
      "batch_index 126\n",
      "batch_index 127\n",
      "batch_index 128\n",
      "batch_index 129\n",
      "batch_index 130\n",
      "batch_index 131\n",
      "batch_index 132\n",
      "batch_index 133\n",
      "batch_index 134\n",
      "batch_index 135\n",
      "batch_index 136\n",
      "batch_index 137\n",
      "batch_index 138\n",
      "batch_index 139\n",
      "batch_index 140\n",
      "batch_index 141\n",
      "batch_index 142\n",
      "batch_index 143\n",
      "batch_index 144\n",
      "batch_index 145\n",
      "batch_index 146\n",
      "batch_index 147\n",
      "batch_index 148\n",
      "batch_index 149\n",
      "batch_index 150\n",
      "batch_index 151\n",
      "batch_index 152\n",
      "batch_index 153\n",
      "batch_index 154\n",
      "batch_index 155\n",
      "batch_index 156\n",
      "batch_index 157\n",
      "batch_index 158\n",
      "batch_index 159\n",
      "batch_index 160\n",
      "batch_index 161\n",
      "batch_index 162\n",
      "batch_index 163\n",
      "batch_index 164\n",
      "batch_index 165\n",
      "batch_index 166\n",
      "batch_index 167\n",
      "batch_index 168\n",
      "batch_index 169\n",
      "batch_index 170\n",
      "batch_index 171\n",
      "batch_index 172\n",
      "batch_index 173\n",
      "batch_index 174\n",
      "batch_index 175\n",
      "batch_index 176\n",
      "batch_index 177\n",
      "batch_index 178\n",
      "batch_index 179\n",
      "batch_index 180\n",
      "batch_index 181\n",
      "batch_index 182\n",
      "batch_index 183\n",
      "batch_index 184\n",
      "batch_index 185\n",
      "batch_index 186\n",
      "batch_index 187\n",
      "batch_index 188\n",
      "batch_index 189\n",
      "batch_index 190\n",
      "batch_index 191\n",
      "batch_index 192\n",
      "batch_index 193\n",
      "batch_index 194\n",
      "batch_index 195\n",
      "batch_index 196\n",
      "batch_index 197\n",
      "batch_index 198\n",
      "batch_index 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 5 loss: 76324.4669342041 time: 240\n",
      "INFO:__main__: right: 209 of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n",
      "batch_index 15\n",
      "batch_index 16\n",
      "batch_index 17\n",
      "batch_index 18\n",
      "batch_index 19\n",
      "batch_index 20\n",
      "batch_index 21\n",
      "batch_index 22\n",
      "batch_index 23\n",
      "batch_index 24\n",
      "batch_index 25\n",
      "batch_index 26\n",
      "batch_index 27\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 7.76 GiB total capacity; 6.57 GiB already allocated; 49.06 MiB free; 59.19 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-01f5a46f4559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mqembedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maembedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-2cbe4ae55c82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mqoptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 7.76 GiB total capacity; 6.57 GiB already allocated; 49.06 MiB free; 59.19 MiB cached)"
     ]
    }
   ],
   "source": [
    "qembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "aembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unlp_startup.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
