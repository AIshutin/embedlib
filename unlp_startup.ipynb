{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZQKX1UAEKw5"
   },
   "source": [
    "## BERT&co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeGAu8raEEIG"
   },
   "source": [
    "ML-часть, распиленная на пайплайны:\n",
    "\n",
    "    Обучатор берта. Возьмём версию на PyTorch от huggingface. Для английского отсутствует — можно взять предобученный от гугла.\n",
    "    Первый дообучатор берта. Обучается на вопросах-ответах как болталка. После этого ответная башня выкидывается и сохранаятся только вопросная — болталка нам не нужна.\n",
    "    Второй дообучатор берта. Обучается ранжировать (если данных совсем много — классифицировать) только вопросы по близости через триплет лосс. Требует много реальной разметки, не обязателен.\n",
    "    Парсер диалоговых данных. Сначала возьмём какой-нибудь ubuntu dialogue corpus, но в будущем нужно будет напарсить какой-нибудь твиттер или реддит и хорошо дообучиться на них.\n",
    "\n",
    "В репозитории ml должны быть скрипты для сбора данных (изначально только wget убунту диалог корпуса) и пайплайн для дообучения берта под диалоги. That's it. Результатом основного скрипта для обучения будут два файла — сериализованная моделька и токенизатор — и, возможно, какие-нибудь скрипты, чтобы их можно было использовать бэкэнду на чистом сервере.\n",
    "\n",
    "За основу имеет смысл взять тот репозиторий от huggingface. ЕМНИП, там токенизатор встроен в модель или куда-то на высоком уровне.\n",
    "\n",
    "Там можно несложными хаками докрутить поверх эмбеддера ещё голову, которая будет делать ранжирование (нужно два раза инициализировать берт — сиамская сеть же, нужны две разные башни). Само обучение будет выглядеть так: нарезать данные формата вопрос-правильный_ответ и засунуть в большой батч (скажем, 64 примера), внутри которого для каждого вопроса все остальные 63 ответа считаются негативными. Векторизовав весь батч и посчитав «матрицу умножения», то есть все попарные скалярные произведения, можно эффективнее считать какой-нибудь лосс для ранжирования (см. презентацию).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "kw1ft9QPEPEq",
    "outputId": "e7fd6a36-abe0-4af4-9e24-b10d0bb3c77d"
   },
   "outputs": [],
   "source": [
    "#!pip3 install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzP_Er1nD-U_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6pylSzmEayc"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LEa1ahtMDur"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "u6bBNFZVEosT",
    "outputId": "c7d9e183-5150-401d-d269-2946b9b7ceae"
   },
   "outputs": [],
   "source": [
    "!./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLTMcV3uJq44"
   },
   "source": [
    "Корпус влезет в оперативную память. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZYPxNIBbWjPV",
    "outputId": "96fe851b-8da1-46b8-f83f-8ccd2af5ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test [link] lets see this too [link]/ end\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    # r'http\\S+'\n",
    "    # r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '[link]', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "\n",
    "print( remove_urls(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf?233/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f/ end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "ok8peEdaN-SW",
    "outputId": "d9a294b3-721a-4388-951e-60207b39bb84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./pretrained-bert-base-uncased/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') #('cpu')\n",
    "bert_type = 'bert-base-uncased'\n",
    "max_seq_len = 512 # BERT-BASE restriction\n",
    "cache_dir = './pretrained-' + bert_type\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0TaLgKywJyBn",
    "outputId": "f059e65f-357e-4dfb-b006-4ba73d63f95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "class UbuntuCorpus(Dataset):\n",
    "  def __init__(self, tokenizer, rootdir='./dialogs'):\n",
    "    super(UbuntuCorpus, self).__init__()\n",
    "    dialogs = []\n",
    "    _cnt = 3000 # debug constant\n",
    "    thr = 175\n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for subdir in os.listdir(rootdir):\n",
    "      for dialog in os.listdir(rootdir + '/' + subdir):\n",
    "        path = rootdir + '/' + subdir +'/' + dialog \n",
    "        with open(path) as tsvfile:\n",
    "          reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "          rows = [(row[1], row[-1]) for row in reader]\n",
    "          replicas = []\n",
    "          authors = set()\n",
    "          author = -1\n",
    "          for row in rows:\n",
    "            if author == row[0]:\n",
    "              replicas[-1].append(row[1])\n",
    "            else:\n",
    "              author = row[0]\n",
    "              authors.add(author)\n",
    "              replicas.append([row[1]])\n",
    "              \n",
    "          '''\n",
    "          Answer replic is a replic without ?\n",
    "          Question replic is a replic with ? followed by answer replic\n",
    "          \n",
    "          Both must be longer than thr (after link replacemenets)\n",
    "          \n",
    "          And due to BERT restrictions both in tokenized form must be shorter than max_seq_len\n",
    "          '''\n",
    "          \n",
    "          for i in range(len(replicas)):\n",
    "            replicas[i] = '[CLS] ' + remove_urls(' '.join(replicas[i]))\n",
    "          \n",
    "          \n",
    "          \n",
    "          for i in range(len(replicas) - 1):\n",
    "            if replicas[i].count('?') > 0 and replicas[i + 1].count('?') == 0 \\\n",
    "              and min(len(replicas[i]), len(replicas[i + 1])) > thr \\\n",
    "              and len(tokenizer.tokenize(replicas[i])) <= max_seq_len \\\n",
    "              and len(tokenizer.tokenize(replicas[i + 1])) <= max_seq_len:\n",
    "              qa_pairs.append([replicas[i], replicas[i + 1]])\n",
    "              _cnt -= 1\n",
    "              if _cnt <=0:\n",
    "                break\n",
    "          \n",
    "          \n",
    "    \n",
    "          #for replica in replicas:\n",
    "          #  print('>>>', replica)\n",
    "          #  print()\n",
    "          #print(authors)\n",
    "          #print()\n",
    "          #print()\n",
    "        \n",
    "        if _cnt <= 0:\n",
    "          break\n",
    "      if _cnt <=0:\n",
    "          break\n",
    "    '''for el in qa_pairs:\n",
    "      print('>>', el[0])\n",
    "      print('>>>', el[1])\n",
    "      print()'''\n",
    "    \n",
    "    self.qa_pairs = qa_pairs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.qa_pairs)\n",
    "  \n",
    "  def __getitem__(self, ind):\n",
    "    return (self.qa_pairs[ind][0], self.qa_pairs[ind][1])#answ)      \n",
    "        \n",
    "corpus = UbuntuCorpus(tokenizer) # full corpus, 1,917,802 qa pairs \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ru79A8XiQxtG",
    "outputId": "c0ca9447-59e3-4e3b-c3a9-e89df057f4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 17 11:21:41 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:06:00.0  On |                  N/A |\r\n",
      "| 20%   36C    P0    44W / 175W |    340MiB /  7949MiB |      6%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1217      G   /usr/lib/xorg/Xorg                            18MiB |\r\n",
      "|    0      1254      G   /usr/bin/gnome-shell                          50MiB |\r\n",
      "|    0      1707      G   /usr/lib/xorg/Xorg                           134MiB |\r\n",
      "|    0      1840      G   /usr/bin/gnome-shell                         132MiB |\r\n",
      "|    0      2998      G   /usr/lib/firefox/firefox                       3MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeSOc5PAcZzO"
   },
   "outputs": [],
   "source": [
    "#pickle.dump(corpus, open( \"./corpus.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fkK2SkiMzeJL",
    "outputId": "9773d9ce-1bf1-49c2-8bb8-2794fd90b6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"[CLS] I am booting in and out, could you please guide me what will be the next step, if that doesn't work, also i got a snapshot of the error message. How can i send it to you ?\", '[CLS] whats the coool gmail app that checks ur gmail mail for linux? :) thanks does everyone have lag when trying to move windows like firefox around in a circle fairly fast or is it just me? running kubuntu , does it have evolution? linux using 2GB of my memory. im shocked no. good to know :) top? im looking at xubuntu system monitor sharing swap drive between ubuntu and kbuntu isnt an issue, right ?'), ('[CLS] [repeating in case you missed it] unclear why it is not working. open a terminal and try the following: \"sudo mkdir /media/EXTDRIVE; sudo mount /dev/sdb1 /media/EXTDRIVE; ls /media/EXTDRIVE\"', \"[CLS] you should share a normal drive.  They overlap by about 80% install ubuntu, then apt-get kubuntu-desktop (or other wayaround) You don't need to dualboot for ubuntu and kubuntu.\")]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "batch = None\n",
    "for el in trainloader:\n",
    "  batch = el\n",
    "  break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nww6TrrdiUmD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_batch(batch):\n",
    "  (quests, answs) = batch\n",
    "  quests = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in quests]\n",
    "  answs = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in answs]\n",
    "  \n",
    "  quest_segments = [torch.tensor([[0 for i in range(len(quests[j]))]]) for j in range(len(quests))]\n",
    "  answ_segments = [torch.tensor([[0 for i in range(len(answs[j] ))]]) for j in range(len(answs))]\n",
    "  \n",
    "  quests = [torch.tensor([el]) for el in quests]\n",
    "  answs = [torch.tensor([el]) for el in answs]\n",
    "  \n",
    "  return ((quests, quest_segments), (answs, answ_segments))\n",
    "  \n",
    "prepare_batch(batch)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ojqkz_tswXP-",
    "outputId": "6417c77d-9a4e-48f3-b1e1-09e7260a7a3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embeddings):\n",
    "  '''\n",
    "  using default bert-as-service strategy to get fixed-size vector\n",
    "  1. considering only -2 layer\n",
    "  2. \"REDUCE_MEAN take the average of the hidden state of encoding layer on the time axis\" @bert-as-service  \n",
    "  '''\n",
    "  embeddings = embeddings[-2]\n",
    "  result = torch.sum(embeddings, dim=1)\n",
    "  \n",
    "  return result.to(device)\n",
    "\n",
    "def embed_batch(batch, qembedder, aembedder):\n",
    "  ((quests, quest_segments), (answs, answ_segments)) = batch\n",
    "  \n",
    "  tmp_quest = [get_embedding(qembedder(quests[i].to(device), quest_segments[i].to(device))[0]) for i in range(len(quests))]\n",
    "  tmp_answ = [get_embedding(aembedder(answs[i].to(device), answ_segments[i].to(device))[0]) for i in range(len(answs))]\n",
    "  \n",
    "  qembeddings = torch.cat(tmp_quest)\n",
    "  aembeddings = torch.cat(tmp_answ)\n",
    "    \n",
    "  return (qembeddings, aembeddings)\n",
    "\n",
    "#embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hD6cPbpDW8nR"
   },
   "outputs": [],
   "source": [
    "def hinge_loss(X, Y, margin=0.1):\n",
    "  batch_size = X.shape[0]\n",
    "  similarities = cosine_similarity_table(X, Y)\n",
    "  #^ см. ниже\n",
    "  \n",
    "  identity = torch.eye(batch_size, device=X.device)\n",
    "  non_diagonal = torch.ones_like(similarities) - identity\n",
    "  \n",
    "  targets = identity - non_diagonal\n",
    "  weights = identity + non_diagonal / (batch_size - 1)\n",
    "  \n",
    "  #всё то же самое, но лосс другой: учитываем только то, что не превосходит margin\n",
    "  losses = torch.pow(F.relu(margin - targets * similarities), 2)\n",
    "  return torch.mean(losses * weights)\n",
    "\n",
    "def cosine_similarity_table(X, Y):\n",
    "  X = F.normalize(X)\n",
    "  Y = F.normalize(Y)\n",
    "  return torch.mm(X, Y.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(X, Y, margin=0.1):\n",
    "    batch_size = X.shape[0]\n",
    "    similarities = cosine_similarity_table(X, Y)\n",
    "    \n",
    "    # anchor - question\n",
    "    # possitive - answer\n",
    "    # negative - wrong answer\n",
    "    \n",
    "    # qa-pair is positive\n",
    "    # q-another a pair is negative\n",
    "    # q-q pair is negative\n",
    "    # a-a pair is negative\n",
    "    \n",
    "    # Approach\n",
    "    # qa versus a\n",
    "    \n",
    "    # L=max(d(a,p)−d(a,n)+margin,0)\n",
    "    right_conf = torch.eye(batch_size, device=X.device) * similarities\n",
    "    max_confq, _ = similarities.max()\n",
    "    max_confa, _ = similarities.max(0)\n",
    "    total_loss = F.relu(max_confq - right_conf + margin) + F.relu(max_confa - right_conf + margin)    \n",
    "    \n",
    "    return total_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a42F6Nb-yEMc"
   },
   "outputs": [],
   "source": [
    "def bce_loss(X, Y, conf_true=0.9, conf_false=0.1): \n",
    "  '''на вход пришел батч размера n,\n",
    "  мы векторизовали контексты (X)\n",
    "  и ответы (Y) и хотим сделать n*n\n",
    "  независимых классификаций\n",
    "  '''\n",
    "  n = X.shape[0]\n",
    "\n",
    "  logits = torch.mm(X, Y.transpose(0, 1)) # считаем таблицу умножения\n",
    "  identity = torch.eye(n, device=X.device)\n",
    "  \n",
    "  non_diagonal = torch.ones_like(logits) - identity\n",
    "  targets = identity * conf_true + non_diagonal * conf_false\n",
    "  #получаем матрицу с conf_true на диагонали и conf_false где-либо ещё\n",
    "  \n",
    "  weights = identity + non_diagonal / (n - 1)\n",
    "  # ^ чтобы не было перекоса в сторону негативов\n",
    "  return F.binary_cross_entropy_with_logits(logits, targets, weights) * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZP70JppTRF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, dtype=torch.uint8)\n",
      "tensor(0, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def calc_acc(X, Y):\n",
    "    '''на вход пришел батч размера n,\n",
    "    мы векторизовали контексты (X)\n",
    "    и ответы (Y)'''\n",
    "    \n",
    "    csim = cosine_similarity_table(X, Y)\n",
    "    confidence, predictions = csim.max(-1)\n",
    "    avg = confidence.mean().item()\n",
    "    predictions = list(predictions.cpu())\n",
    "    right = 0\n",
    "    for i in range(len(predictions)):\n",
    "        right += predictions[i] == i\n",
    "    return right\n",
    "    \n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))\n",
    "\n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[.1, 0], [0, .1]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YiTAp29kXV6-",
    "outputId": "05069111-7709-42a9-af7f-13377c234779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_optimizer_params(model):\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "  \n",
    "  optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "  ]\n",
    "  \n",
    "  return optimizer_grouped_parameters\n",
    "\n",
    "#get_optimizer_params(qembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(corpus) * .33)\n",
    "train_size = len(corpus) - test_size\n",
    "train_corpus, test_corpus = torch.utils.data.random_split(corpus, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  batch_size = 10 # 16, 32 are recommended in the paper\n",
    "  trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True) #\n",
    "  testloader = trainloader # DataLoader(test_corpus, batch_size=batch_size, shuffle=True)\n",
    "  num_train_optimization_steps = len(corpus) * epochs\n",
    "  \n",
    "  '''\n",
    "  optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=5e-5,\n",
    "                                 warmup=0.1,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  lr = 5e-5 # 5e-5, 3e-5, 2e-5 are recommended in the paper\n",
    "  warmup = 0.1\n",
    "  \n",
    "  qoptim = BertAdam(get_optimizer_params(qembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_total=num_train_optimization_steps)\n",
    "  aoptim = BertAdam(get_optimizer_params(aembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_toal=num_train_optimization_steps)\n",
    "  criterion = triplet_loss #hinge_loss\n",
    "  \n",
    "  total = right = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        total += len(batch[0])\n",
    "        embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "        right += calc_acc(*embeddings) \n",
    "\n",
    "  qembedder.train()\n",
    "  aembedder.train()\n",
    "  \n",
    "  logger.info(\"***** Running training *****\")\n",
    "  logger.info(\"  Num steps = %d\", num_train_optimization_steps)  \n",
    "  logger.info(f\"Before training: right: {right} of {total}\")\n",
    "  \n",
    "  start_training = time.time()\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    qembedder.train()\n",
    "    aembedder.train()\n",
    "    for bidx, batch in enumerate(tqdm_notebook(iter(trainloader), desc=f\"epoch {epoch}\")):\n",
    "      qoptim.zero_grad()\n",
    "      aoptim.zero_grad()\n",
    "      #print('batch_index', bidx)\n",
    "      embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "      loss = bce_loss(*embeddings)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "\n",
    "      qoptim.step()\n",
    "      aoptim.step()\n",
    "    \n",
    "    total = right = 0\n",
    "    qembedder.eval()\n",
    "    aembedder.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            total += len(batch[0])\n",
    "            embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "            right += calc_acc(*embeddings) \n",
    "\n",
    "    logger.info(f'right: {right} of {total} | loss: {total_loss} ')\n",
    "    \n",
    "  end_training = time.time()\n",
    "  logger.info(f'Training is compleated. Time: {int(end_training - start_training)}')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "4lPnOyLCLuj_",
    "outputId": "96b3b23e-afce-45e7-efaf-f84c3e5a5fb1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmptn1sv71w\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpr4la3lqf\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num steps = 6000\n",
      "INFO:__main__:Before training: right: 18 of 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052c825974b4450aa1e6ce735d0c8fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=300, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:right: 48 of 3000 | loss: 3689953.2194213867 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440ac1f265a4459c846a3fd7d837293c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=300, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:right: 38 of 3000 | loss: 391809.34756469727 \n",
      "INFO:__main__:Training is compleated. Time: 647\n"
     ]
    }
   ],
   "source": [
    "qembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "aembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "train(2) # 2, 3, 4 epochs are recommended in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unlp_startup.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
