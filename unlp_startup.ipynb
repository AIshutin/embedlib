{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZQKX1UAEKw5"
   },
   "source": [
    "## BERT&co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeGAu8raEEIG"
   },
   "source": [
    "ML-часть, распиленная на пайплайны:\n",
    "\n",
    "    Обучатор берта. Возьмём версию на PyTorch от huggingface. Для английского отсутствует — можно взять предобученный от гугла.\n",
    "    Первый дообучатор берта. Обучается на вопросах-ответах как болталка. После этого ответная башня выкидывается и сохранаятся только вопросная — болталка нам не нужна.\n",
    "    Второй дообучатор берта. Обучается ранжировать (если данных совсем много — классифицировать) только вопросы по близости через триплет лосс. Требует много реальной разметки, не обязателен.\n",
    "    Парсер диалоговых данных. Сначала возьмём какой-нибудь ubuntu dialogue corpus, но в будущем нужно будет напарсить какой-нибудь твиттер или реддит и хорошо дообучиться на них.\n",
    "\n",
    "В репозитории ml должны быть скрипты для сбора данных (изначально только wget убунту диалог корпуса) и пайплайн для дообучения берта под диалоги. That's it. Результатом основного скрипта для обучения будут два файла — сериализованная моделька и токенизатор — и, возможно, какие-нибудь скрипты, чтобы их можно было использовать бэкэнду на чистом сервере.\n",
    "\n",
    "За основу имеет смысл взять тот репозиторий от huggingface. ЕМНИП, там токенизатор встроен в модель или куда-то на высоком уровне.\n",
    "\n",
    "Там можно несложными хаками докрутить поверх эмбеддера ещё голову, которая будет делать ранжирование (нужно два раза инициализировать берт — сиамская сеть же, нужны две разные башни). Само обучение будет выглядеть так: нарезать данные формата вопрос-правильный_ответ и засунуть в большой батч (скажем, 64 примера), внутри которого для каждого вопроса все остальные 63 ответа считаются негативными. Векторизовав весь батч и посчитав «матрицу умножения», то есть все попарные скалярные произведения, можно эффективнее считать какой-нибудь лосс для ранжирования (см. презентацию).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "kw1ft9QPEPEq",
    "outputId": "e7fd6a36-abe0-4af4-9e24-b10d0bb3c77d"
   },
   "outputs": [],
   "source": [
    "#!pip3 install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzP_Er1nD-U_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6pylSzmEayc"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LEa1ahtMDur"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "u6bBNFZVEosT",
    "outputId": "c7d9e183-5150-401d-d269-2946b9b7ceae"
   },
   "outputs": [],
   "source": [
    "!./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLTMcV3uJq44"
   },
   "source": [
    "Корпус влезет в оперативную память. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZYPxNIBbWjPV",
    "outputId": "96fe851b-8da1-46b8-f83f-8ccd2af5ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test [link] lets see this too [link] end\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '[link]', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "\n",
    "print( remove_urls(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf?233/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "ok8peEdaN-SW",
    "outputId": "d9a294b3-721a-4388-951e-60207b39bb84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./pretrained-bert-base-uncased/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmplc93atv_\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file ./pretrained-bert-base-uncased/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpw3dnuokc\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') #('cpu')\n",
    "bert_type = 'bert-base-uncased'\n",
    "max_seq_len = 512 # BERT-BASE restriction\n",
    "cache_dir = './pretrained-' + bert_type\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type, cache_dir=cache_dir)\n",
    "qembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "aembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0TaLgKywJyBn",
    "outputId": "f059e65f-357e-4dfb-b006-4ba73d63f95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "class UbuntuCorpus(Dataset):\n",
    "  def __init__(self, tokenizer, rootdir='./dialogs'):\n",
    "    super(UbuntuCorpus, self).__init__()\n",
    "    dialogs = []\n",
    "    _cnt = 300 # debug constant\n",
    "    \n",
    "    # punctuations signs after which we put [SEP] token\n",
    "    punctuation_seps = ['?!', '!?', '?', '...', '. '] \n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for subdir in os.listdir(rootdir):\n",
    "      for dialog in os.listdir(rootdir + '/' + subdir):\n",
    "        path = rootdir + '/' + subdir +'/' + dialog \n",
    "        with open(path) as tsvfile:\n",
    "          reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "          rows = [(row[1], row[-1]) for row in reader]\n",
    "          replicas = []\n",
    "          authors = set()\n",
    "          author = -1\n",
    "          for row in rows:\n",
    "            if author == row[0]:\n",
    "              replicas[-1].append(row[1])\n",
    "            else:\n",
    "              author = row[0]\n",
    "              authors.add(author)\n",
    "              replicas.append([row[1]])\n",
    "              \n",
    "          '''\n",
    "          Answer replic is a replic without ?\n",
    "          Question replic is a replic with ? followed by answer replic\n",
    "          \n",
    "          Both must be longer than thr (after link replacemenets)\n",
    "          \n",
    "          And due to BERT restrictions in tokenized form shorter than max_seq_len\n",
    "          '''\n",
    "          \n",
    "          for i in range(len(replicas)):\n",
    "            replicas[i] = '[CLS] ' + remove_urls(' '.join(replicas[i]))\n",
    "            \n",
    "            codephrase = 'evilcyborgswillkillhumanity'\n",
    "            \n",
    "            sep_token = '[SEP]'\n",
    "            \n",
    "            for (ind, el) in enumerate(punctuation_seps):\n",
    "              crouch = codephrase + f'{ind} ' + sep_token + ' '\n",
    "              replicas[i] = replicas[i].replace(el, crouch)\n",
    "            \n",
    "            for (ind, el) in enumerate(punctuation_seps):\n",
    "              replicas[i] = replicas[i].replace(codephrase + f'{ind}', el)\n",
    "              \n",
    "            \n",
    "            if replicas[i].rstrip()[-len(sep_token):] != sep_token:\n",
    "              replicas[i] = replicas[i] + ' ' + sep_token\n",
    "          \n",
    "          thr = 90\n",
    "          \n",
    "          for i in range(len(replicas) - 1):\n",
    "            if replicas[i].count('?') > 0 and replicas[i + 1].count('?') == 0 \\\n",
    "              and min(len(replicas[i]), len(replicas[i + 1])) > thr \\\n",
    "              and len(tokenizer.tokenize(replicas[i])) <= max_seq_len \\\n",
    "              and len(tokenizer.tokenize(replicas[i + 1])) <= max_seq_len:\n",
    "              qa_pairs.append([replicas[i], replicas[i + 1]])\n",
    "              _cnt -= 1\n",
    "              if _cnt <=0:\n",
    "                break\n",
    "          \n",
    "          \n",
    "    \n",
    "          #for replica in replicas:\n",
    "          #  print('>>>', replica)\n",
    "          #  print()\n",
    "          #print(authors)\n",
    "          #print()\n",
    "          #print()\n",
    "        \n",
    "        if _cnt <= 0:\n",
    "          break\n",
    "      if _cnt <=0:\n",
    "          break\n",
    "    '''for el in qa_pairs:\n",
    "      print('>>', el[0])\n",
    "      print('>>>', el[1])\n",
    "      print()'''\n",
    "    \n",
    "    self.qa_pairs = qa_pairs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.qa_pairs)\n",
    "  \n",
    "  def __getitem__(self, ind):\n",
    "    '''\n",
    "    #self.tokenizer = ??\n",
    "    answ = [self.qa_pairs[ind][1]]\n",
    "    used = {ind}\n",
    "    while len(answ) != batch_size:\n",
    "      ind2 = random.randint(0, len(self) - 1)     \n",
    "      if ind2 not in used:\n",
    "        used.add(ind2)\n",
    "        answ.append(self.qa_pairs[ind2][1])\n",
    "    #print('#%^^&', answ)\n",
    "    '''\n",
    "    return (self.qa_pairs[ind][0], self.qa_pairs[ind][1])#answ)      \n",
    "        \n",
    "corpus = UbuntuCorpus(tokenizer) # full corpus, 1,917,802 qa pairs \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ru79A8XiQxtG",
    "outputId": "c0ca9447-59e3-4e3b-c3a9-e89df057f4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 15 12:25:10 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    On   | 00000000:06:00.0  On |                  N/A |\r\n",
      "| 24%   40C    P2    43W / 175W |   1970MiB /  7949MiB |      5%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1170      G   /usr/lib/xorg/Xorg                            18MiB |\r\n",
      "|    0      1211      G   /usr/bin/gnome-shell                          50MiB |\r\n",
      "|    0      1539      G   /usr/lib/xorg/Xorg                           114MiB |\r\n",
      "|    0      1790      G   /usr/bin/gnome-shell                          97MiB |\r\n",
      "|    0      3583      C   /usr/bin/python3                            1677MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeSOc5PAcZzO"
   },
   "outputs": [],
   "source": [
    "#pickle.dump(corpus, open( \"./corpus.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "UTMFxa1Ndslr",
    "outputId": "6d8abb1f-750e-46cb-f505-de6dbfc0f853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/aishutin/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ah, sorry.  [SEP] as in you can't hear anything or as in sound programs throw errors? [SEP] \n",
      "['[CLS]', 'ah', ',', 'sorry', '.', '[SEP]', 'as', 'in', 'you', 'can', \"'\", 't', 'hear', 'anything', 'or', 'as', 'in', 'sound', 'programs', 'throw', 'errors', '?', '[SEP]']\n",
      "----------------\n",
      "[CLS] it doesnt throw error... [SEP] just cant listen... [SEP] i open the volume thingy... [SEP] and i can see it sounds... [SEP] but i cant hear shit... [SEP] and all the volumes are up [SEP]\n"
     ]
    }
   ],
   "source": [
    "item = corpus[random.randint(0, len(corpus) - 1)]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#print(item)\n",
    "print(item[0])\n",
    "print(tokenizer.tokenize(item[0]))\n",
    "print('----------------')\n",
    "\n",
    "print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fkK2SkiMzeJL",
    "outputId": "9773d9ce-1bf1-49c2-8bb8-2794fd90b6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS] i dont know of dhcp, i know of dhclient as dhcp client, dhcp3 is the name of a dhcpd server? [SEP] ', '[CLS] this is from Breezy install? [SEP]  which hard disk did you install to? [SEP]  ah, here we have a communication issue.  [SEP]  I asked if you have installed it already [SEP]'), (\"[CLS] : yes... [SEP]  it's what it says in the synaptic... [SEP]  : and it's marked with ubuntu logo... [SEP] \", '[CLS] oh, yeah im trying to get it on the external but like when i ran the install disc earlier i couldnt see my external in the partitino stage.  [SEP] so i came on the live cd(still on it) and started trying to figure it out [SEP]')]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "batch = None\n",
    "for el in trainloader:\n",
    "  batch = el\n",
    "  break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nww6TrrdiUmD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_batch(batch):\n",
    "  (quests, answs) = batch\n",
    "  quests = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in quests]\n",
    "  answs = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in answs]\n",
    "  \n",
    "  quest_segments = [torch.tensor([[0 for i in range(len(quests[j]))]]) for j in range(len(quests))]\n",
    "  answ_segments = [torch.tensor([[0 for i in range(len(answs[j] ))]]) for j in range(len(answs))]\n",
    "  \n",
    "  quests = [torch.tensor([el]) for el in quests]\n",
    "  answs = [torch.tensor([el]) for el in answs]\n",
    "  \n",
    "  return ((quests, quest_segments), (answs, answ_segments))\n",
    "  \n",
    "  #  torch.tensor(\n",
    "  \n",
    "prepare_batch(batch)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ojqkz_tswXP-",
    "outputId": "6417c77d-9a4e-48f3-b1e1-09e7260a7a3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embeddings):\n",
    "  '''\n",
    "  using default bert-as-service strategy to get fixed-size vector\n",
    "  1. considering only -2 layer\n",
    "  2. \"REDUCE_MEAN \ttake the average of the hidden state of encoding layer on the time axis\" @bert-as-service  \n",
    "  '''\n",
    "  embeddings = embeddings[-2]\n",
    "  result = torch.sum(embeddings, dim=1)\n",
    "   \n",
    "  #print(embeddings.shape)\n",
    "  #print(result.shape)\n",
    "  \n",
    "  return result.to(device)\n",
    "\n",
    "def embed_batch(batch, qembedder, aembedder, grad=True):\n",
    "  assert(grad)\n",
    "  ((quests, quest_segments), (answs, answ_segments)) = batch\n",
    "  \n",
    "  #print(quests[0])\n",
    "  #print(quest_segments[0])\n",
    "  \n",
    "  tmp_quest = [get_embedding(qembedder(quests[i].to(device), quest_segments[i].to(device))[0]) for i in range(len(quests))]\n",
    "  \n",
    "  tmp_answ = [get_embedding(aembedder(answs[i].to(device), answ_segments[i].to(device))[0]) for i in range(len(answs))]\n",
    "  \n",
    "  aembeddings = torch.cat(tmp_answ)\n",
    "  qembeddings = torch.cat(tmp_quest)\n",
    "    \n",
    "  return (qembeddings, aembeddings)\n",
    "\n",
    "embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hD6cPbpDW8nR"
   },
   "outputs": [],
   "source": [
    "def hinge_loss(X, Y, margin=0.1):\n",
    "  batch_size = X.shape[0]\n",
    "  similarities = cosine_similarity_table(X, Y)\n",
    "  #^ см. ниже\n",
    "  \n",
    "  identity = torch.eye(batch_size, device=X.device)\n",
    "  non_diagonal = torch.ones_like(similarities) - identity\n",
    "  \n",
    "  targets = identity - non_diagonal\n",
    "  weights = identity + non_diagonal / (batch_size - 1)\n",
    "  \n",
    "  #всё то же самое, но лосс другой: учитываем только то, что не превосходит margin\n",
    "  losses = torch.pow(F.relu(margin - targets * similarities), 2)\n",
    "  return torch.mean(losses * weights)\n",
    "\n",
    "def cosine_similarity_table(X, Y):\n",
    "  X = F.normalize(X)\n",
    "  Y = F.normalize(Y)\n",
    "  return torch.mm(X, Y.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a42F6Nb-yEMc"
   },
   "outputs": [],
   "source": [
    "def bce_loss(X, Y, conf_true=0.9, conf_false=0.1): \n",
    "  '''на вход пришел батч размера n,\n",
    "  мы векторизовали контексты (X)\n",
    "  и ответы (Y) и хотим сделать n*n\n",
    "  независимых классификаций\n",
    "  '''\n",
    "  n = X.shape[0]\n",
    "\n",
    "  logits = torch.mm(X, Y.transpose(0, 1)) # считаем таблицу умножения\n",
    "  identity = torch.eye(n, device=X.device)\n",
    "  \n",
    "  non_diagonal = torch.ones_like(logits) - identity\n",
    "  targets = identity * conf_true + non_diagonal * conf_false\n",
    "  #получаем матрицу с conf_true на диагонали и conf_false где-либо ещё\n",
    "  \n",
    "  weights = identity + non_diagonal / (n - 1)\n",
    "  # ^ чтобы не было перекоса в сторону негативов\n",
    "  return F.binary_cross_entropy_with_logits(logits, targets, weights) * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZP70JppTRF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, dtype=torch.uint8)\n",
      "tensor(0, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def calc_acc(X, Y):\n",
    "    '''на вход пришел батч размера n,\n",
    "    мы векторизовали контексты (X)\n",
    "    и ответы (Y)'''\n",
    "    \n",
    "    csim = cosine_similarity_table(X, Y)\n",
    "    confidence, predictions = csim.max(-1)\n",
    "    avg = confidence.mean().item()\n",
    "    predictions = list(predictions.cpu())\n",
    "    right = 0\n",
    "    for i in range(len(predictions)):\n",
    "        right += predictions[i] == i\n",
    "    return right\n",
    "    \n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))\n",
    "\n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[.1, 0], [0, .1]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YiTAp29kXV6-",
    "outputId": "05069111-7709-42a9-af7f-13377c234779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_optimizer_params(model):\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "  \n",
    "  optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "  ]\n",
    "  \n",
    "  return optimizer_grouped_parameters\n",
    "\n",
    "get_optimizer_params(qembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(corpus) * .33)\n",
    "train_size = len(corpus) - test_size\n",
    "train_corpus, test_corpus = torch.utils.data.random_split(corpus, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uaoHFeKixBNP"
   },
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  batch_size = 20\n",
    "  trainloader = DataLoader(train_corpus, batch_size=batch_size, shuffle=True)\n",
    "  testloader = DataLoader(test_corpus, batch_size=batch_size, shuffle=True)\n",
    "  num_train_optimization_steps = len(train_corpus) * epochs\n",
    "  \n",
    "  '''\n",
    "  optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=5e-5,\n",
    "                                 warmup=0.1,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  lr = 5e-5\n",
    "  warmup = 0.1\n",
    "  \n",
    "  qoptim = BertAdam(get_optimizer_params(qembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_total=num_train_optimization_steps)\n",
    "  aoptim = BertAdam(get_optimizer_params(aembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_toal=num_train_optimization_steps)\n",
    "  criterion = hinge_loss\n",
    "  \n",
    "  qembedder.train()\n",
    "  aembedder.train()\n",
    "  \n",
    "  logger.info(\"***** Running training *****\")\n",
    "  logger.info(\"  Num steps = %d\", num_train_optimization_steps)  \n",
    "  \n",
    "  total = 0\n",
    "  right = 0\n",
    "\n",
    "  '''with torch.no_grad():\n",
    "    for batch in trainloader:\n",
    "        total += len(batch[0])\n",
    "        embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "        right += calc_acc(*embeddings)    '''\n",
    "  logger.info(f'Before fine-tuning')\n",
    "  logger.info(f'right {right} from {total}')\n",
    "\n",
    "  start_training = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    start_epoch = time.time()\n",
    "    for bidx, batch in enumerate(trainloader):\n",
    "      qoptim.zero_grad()\n",
    "      aoptim.zero_grad()\n",
    "      print('batch_index', bidx)\n",
    "      embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "      loss = bce_loss(*embeddings)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "\n",
    "      qoptim.step()\n",
    "      aoptim.step()\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    qoptim.zero_grad()\n",
    "    aoptim.zero_grad()\n",
    "    \n",
    "    total = 0\n",
    "    right = 0\n",
    "    '''with torch.no_grad():\n",
    "        for batch in trainloader:\n",
    "            total += len(batch[0])\n",
    "            embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "            right += calc_acc(*embeddings)    '''\n",
    "    logger.info(f'epoch {epoch} loss: {total_loss} time: {int(end_epoch - start_epoch)}')\n",
    "    logger.info(f'right {right} from {total}')\n",
    "    \n",
    "  end_training = time.time()\n",
    "  logger.info(f'Training is compleated. Total time: {int(end_training - start_training)}')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  batch_size = 20\n",
    "  trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "  num_train_optimization_steps = len(corpus) * epochs\n",
    "  \n",
    "  '''\n",
    "  optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                                 lr=5e-5,\n",
    "                                 warmup=0.1,\n",
    "                                 t_total=num_train_optimization_steps)\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  lr = 5e-5\n",
    "  warmup = 0.1\n",
    "  \n",
    "  qoptim = BertAdam(get_optimizer_params(qembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_total=num_train_optimization_steps)\n",
    "  aoptim = BertAdam(get_optimizer_params(aembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_toal=num_train_optimization_steps)\n",
    "  criterion = hinge_loss\n",
    "  \n",
    "  total = right = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in trainloader:\n",
    "        total += len(batch[0])\n",
    "        embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "        right += calc_acc(*embeddings) \n",
    "\n",
    "  qembedder.train()\n",
    "  aembedder.train()\n",
    "  \n",
    "  logger.info(\"***** Running training *****\")\n",
    "  logger.info(\"  Num steps = %d\", num_train_optimization_steps)  \n",
    "  logger.info(f\" right: {right} of {total}\")\n",
    "  \n",
    "  start_training = time.time()\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    start_epoch = time.time()\n",
    "    for bidx, batch in enumerate(trainloader):\n",
    "      qoptim.zero_grad()\n",
    "      aoptim.zero_grad()\n",
    "      print('batch_index', bidx)\n",
    "      embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "      loss = bce_loss(*embeddings)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "\n",
    "      qoptim.step()\n",
    "      aoptim.step()\n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    \n",
    "    total = right = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in trainloader:\n",
    "            total += len(batch[0])\n",
    "            embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "            right += calc_acc(*embeddings) \n",
    "\n",
    "    logger.info(f'epoch {epoch} loss: {total_loss} time: {int(end_epoch - start_epoch)}')\n",
    "    logger.info(f\" right: {right} of {total}\")\n",
    "    \n",
    "  end_training = time.time()\n",
    "  logger.info(f'Training is compleated time: {int(end_training - start_training)}')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "4lPnOyLCLuj_",
    "outputId": "96b3b23e-afce-45e7-efaf-f84c3e5a5fb1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num steps = 3000\n",
      "INFO:__main__: right: 46 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 0 loss: 765744.4918212891 time: 22\n",
      "INFO:__main__: right: 14 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 1 loss: 20190.418090820312 time: 22\n",
      "INFO:__main__: right: 17 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 2 loss: 16312.077545166016 time: 22\n",
      "INFO:__main__: right: 16 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 3 loss: 15739.087341308594 time: 22\n",
      "INFO:__main__: right: 15 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 4 loss: 13532.120727539062 time: 22\n",
      "INFO:__main__: right: 12 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 5 loss: 10836.67593383789 time: 22\n",
      "INFO:__main__: right: 19 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 6 loss: 9331.0458984375 time: 22\n",
      "INFO:__main__: right: 14 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 7 loss: 9011.382202148438 time: 22\n",
      "INFO:__main__: right: 18 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 8 loss: 11054.727416992188 time: 22\n",
      "INFO:__main__: right: 13 of 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_index 0\n",
      "batch_index 1\n",
      "batch_index 2\n",
      "batch_index 3\n",
      "batch_index 4\n",
      "batch_index 5\n",
      "batch_index 6\n",
      "batch_index 7\n",
      "batch_index 8\n",
      "batch_index 9\n",
      "batch_index 10\n",
      "batch_index 11\n",
      "batch_index 12\n",
      "batch_index 13\n",
      "batch_index 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:epoch 9 loss: 8615.00668334961 time: 22\n",
      "INFO:__main__: right: 19 of 300\n",
      "INFO:__main__:Training is compleated time: 304\n"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unlp_startup.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
