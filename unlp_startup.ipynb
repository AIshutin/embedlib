{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZQKX1UAEKw5"
   },
   "source": [
    "## BERT&co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeGAu8raEEIG"
   },
   "source": [
    "ML-часть, распиленная на пайплайны:\n",
    "\n",
    "    Обучатор берта. Возьмём версию на PyTorch от huggingface. Для английского отсутствует — можно взять предобученный от гугла.\n",
    "    Первый дообучатор берта. Обучается на вопросах-ответах как болталка. После этого ответная башня выкидывается и сохранаятся только вопросная — болталка нам не нужна.\n",
    "    Второй дообучатор берта. Обучается ранжировать (если данных совсем много — классифицировать) только вопросы по близости через триплет лосс. Требует много реальной разметки, не обязателен.\n",
    "    Парсер диалоговых данных. Сначала возьмём какой-нибудь ubuntu dialogue corpus, но в будущем нужно будет напарсить какой-нибудь твиттер или реддит и хорошо дообучиться на них.\n",
    "\n",
    "В репозитории ml должны быть скрипты для сбора данных (изначально только wget убунту диалог корпуса) и пайплайн для дообучения берта под диалоги. That's it. Результатом основного скрипта для обучения будут два файла — сериализованная моделька и токенизатор — и, возможно, какие-нибудь скрипты, чтобы их можно было использовать бэкэнду на чистом сервере.\n",
    "\n",
    "За основу имеет смысл взять тот репозиторий от huggingface. ЕМНИП, там токенизатор встроен в модель или куда-то на высоком уровне.\n",
    "\n",
    "Там можно несложными хаками докрутить поверх эмбеддера ещё голову, которая будет делать ранжирование (нужно два раза инициализировать берт — сиамская сеть же, нужны две разные башни). Само обучение будет выглядеть так: нарезать данные формата вопрос-правильный_ответ и засунуть в большой батч (скажем, 64 примера), внутри которого для каждого вопроса все остальные 63 ответа считаются негативными. Векторизовав весь батч и посчитав «матрицу умножения», то есть все попарные скалярные произведения, можно эффективнее считать какой-нибудь лосс для ранжирования (см. презентацию).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "kw1ft9QPEPEq",
    "outputId": "e7fd6a36-abe0-4af4-9e24-b10d0bb3c77d"
   },
   "outputs": [],
   "source": [
    "#!pip3 install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzP_Er1nD-U_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForQuestionAnswering, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from bert_serving.client import BertClient\n",
    "import csv\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6pylSzmEayc"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LEa1ahtMDur"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "u6bBNFZVEosT",
    "outputId": "c7d9e183-5150-401d-d269-2946b9b7ceae"
   },
   "outputs": [],
   "source": [
    "!./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLTMcV3uJq44"
   },
   "source": [
    "Корпус влезет в оперативную память. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZYPxNIBbWjPV",
    "outputId": "96fe851b-8da1-46b8-f83f-8ccd2af5ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test [link] lets see this too [link]/ end\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    # r'http\\S+'\n",
    "    # r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '[link]', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "\n",
    "print( remove_urls(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf?233/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f/ end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "ok8peEdaN-SW",
    "outputId": "d9a294b3-721a-4388-951e-60207b39bb84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./pretrained-bert-base-uncased/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') #('cpu')\n",
    "bert_type = 'bert-base-uncased'\n",
    "max_seq_len = 512 # BERT-BASE restriction\n",
    "cache_dir = './pretrained-' + bert_type\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0TaLgKywJyBn",
    "outputId": "f059e65f-357e-4dfb-b006-4ba73d63f95c"
   },
   "outputs": [],
   "source": [
    "class UbuntuCorpus(Dataset):\n",
    "  def __init__(self, tokenizer, rootdir='./dialogs'):\n",
    "    super(UbuntuCorpus, self).__init__()\n",
    "    dialogs = []\n",
    "    _cnt = 3000 # debug constant\n",
    "    thr = 175\n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for subdir in os.listdir(rootdir):\n",
    "      for dialog in os.listdir(rootdir + '/' + subdir):\n",
    "        path = rootdir + '/' + subdir +'/' + dialog \n",
    "        with open(path) as tsvfile:\n",
    "          reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "          rows = [(row[1], row[-1]) for row in reader]\n",
    "          replicas = []\n",
    "          authors = set()\n",
    "          author = -1\n",
    "          for row in rows:\n",
    "            if author == row[0]:\n",
    "              replicas[-1].append(row[1])\n",
    "            else:\n",
    "              author = row[0]\n",
    "              authors.add(author)\n",
    "              replicas.append([row[1]])\n",
    "              \n",
    "          '''\n",
    "          Answer replic is a replic without ?\n",
    "          Question replic is a replic with ? followed by answer replic\n",
    "          \n",
    "          Both must be longer than thr (after link replacemenets)\n",
    "          \n",
    "          And due to BERT restrictions both in tokenized form must be shorter than max_seq_len\n",
    "          '''\n",
    "          \n",
    "          for i in range(len(replicas)):\n",
    "            replicas[i] = '[CLS] ' + remove_urls(' '.join(replicas[i]))\n",
    "          \n",
    "          \n",
    "          \n",
    "          for i in range(len(replicas) - 1):\n",
    "            if replicas[i].count('?') > 0 and replicas[i + 1].count('?') == 0 \\\n",
    "              and min(len(replicas[i]), len(replicas[i + 1])) >= thr \\\n",
    "              and len(tokenizer.tokenize(replicas[i])) <= max_seq_len \\\n",
    "              and len(tokenizer.tokenize(replicas[i + 1])) <= max_seq_len:\n",
    "              qa_pairs.append([replicas[i], replicas[i + 1]])\n",
    "              _cnt -= 1\n",
    "              if _cnt <=0:\n",
    "                break\n",
    "          \n",
    "          \n",
    "    \n",
    "          #for replica in replicas:\n",
    "          #  print('>>>', replica)\n",
    "          #  print()\n",
    "          #print(authors)\n",
    "          #print()\n",
    "          #print()\n",
    "        \n",
    "        if _cnt <= 0:\n",
    "          break\n",
    "      if _cnt <=0:\n",
    "          break\n",
    "    '''for el in qa_pairs:\n",
    "      print('>>', el[0])\n",
    "      print('>>>', el[1])\n",
    "      print()'''\n",
    "    \n",
    "    self.qa_pairs = qa_pairs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.qa_pairs)\n",
    "  \n",
    "  def __getitem__(self, ind):\n",
    "    return (self.qa_pairs[ind][0], self.qa_pairs[ind][1])#answ)      \n",
    "        \n",
    "corpus = UbuntuCorpus(tokenizer) # full corpus, 1,917,802 qa pairs \n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ru79A8XiQxtG",
    "outputId": "c0ca9447-59e3-4e3b-c3a9-e89df057f4cf"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeSOc5PAcZzO"
   },
   "outputs": [],
   "source": [
    "#pickle.dump(corpus, open( \"./corpus.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fkK2SkiMzeJL",
    "outputId": "9773d9ce-1bf1-49c2-8bb8-2794fd90b6c4"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True)\n",
    "batch = None\n",
    "for el in trainloader:\n",
    "  batch = el\n",
    "  break\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nww6TrrdiUmD"
   },
   "outputs": [],
   "source": [
    "def prepare_batch(batch):\n",
    "  (quests, answs) = batch\n",
    "  #print('prepare_batch', quests)\n",
    "  quests = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in quests]\n",
    "  #print('tokenized', quests)\n",
    "  answs = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(el)) for el in answs]\n",
    "  \n",
    "  quest_segments = [torch.tensor([[0 for i in range(len(quests[j]))]]) for j in range(len(quests))]\n",
    "  answ_segments = [torch.tensor([[0 for i in range(len(answs[j] ))]]) for j in range(len(answs))]\n",
    "  \n",
    "  quests = [torch.tensor([el]) for el in quests]\n",
    "  answs = [torch.tensor([el]) for el in answs]\n",
    "  \n",
    "  return ((quests, quest_segments), (answs, answ_segments))\n",
    "\n",
    "prepare_batch(batch)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ojqkz_tswXP-",
    "outputId": "6417c77d-9a4e-48f3-b1e1-09e7260a7a3f"
   },
   "outputs": [],
   "source": [
    "def get_embedding(embeddings):\n",
    "  '''\n",
    "  using default bert-as-service strategy to get fixed-size vector\n",
    "  1. considering only -2 layer\n",
    "  2. \"REDUCE_MEAN take the average of the hidden state of encoding layer on the time axis\" @bert-as-service  \n",
    "  '''\n",
    "  embeddings = embeddings[-2]\n",
    "  '''lst = []\n",
    "  for i in range(embeddings.shape[1]):\n",
    "      lst.append(embeddings[0][i][0].item())\n",
    "  print('get embedding', lst)'''\n",
    "  result = torch.mean(embeddings, dim=1)\n",
    "  \n",
    "  return result.to(device)\n",
    "\n",
    "def embed_batch(batch, qembedder, aembedder):\n",
    "  ((quests, quest_segments), (answs, answ_segments)) = batch\n",
    "  #print('embed_batch', quests)\n",
    "    \n",
    "  tmp_quest = [get_embedding(qembedder(quests[i].to(device), quest_segments[i].to(device))[0]) for i in range(len(quests))]\n",
    "  tmp_answ = [get_embedding(aembedder(answs[i].to(device), answ_segments[i].to(device))[0]) for i in range(len(answs))]\n",
    "  \n",
    "  qembeddings = torch.cat(tmp_quest)\n",
    "  aembeddings = torch.cat(tmp_answ)\n",
    "    \n",
    "  return (qembeddings, aembeddings)\n",
    "\n",
    "#embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "text = ['[CLS] First do it']\n",
    "qembedder.eval()\n",
    "embeddings = embed_batch(prepare_batch((text, text)), qembedder, qembedder)[0]\n",
    "\n",
    "embeddings = list(embeddings)\n",
    "for el in embeddings:\n",
    "    print(el)\n",
    "    print()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hD6cPbpDW8nR"
   },
   "outputs": [],
   "source": [
    "def hinge_loss(X, Y, margin=0.1):\n",
    "  batch_size = X.shape[0]\n",
    "  similarities = cosine_similarity_table(X, Y)\n",
    "  #^ см. ниже\n",
    "  \n",
    "  identity = torch.eye(batch_size, device=X.device)\n",
    "  non_diagonal = torch.ones_like(similarities) - identity\n",
    "  \n",
    "  targets = identity - non_diagonal\n",
    "  weights = identity + non_diagonal / (batch_size - 1)\n",
    "  \n",
    "  #всё то же самое, но лосс другой: учитываем только то, что не превосходит margin\n",
    "  losses = torch.pow(F.relu(margin - targets * similarities), 2)\n",
    "  return torch.mean(losses * weights)\n",
    "\n",
    "def cosine_similarity_table(X, Y):\n",
    "  X = F.normalize(X)\n",
    "  Y = F.normalize(Y)\n",
    "  return torch.mm(X, Y.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(X, Y, margin=0.1):\n",
    "    # https://omoindrot.github.io/triplet-loss\n",
    "    \n",
    "    batch_size = X.shape[0]\n",
    "    similarities = cosine_similarity_table(X, Y)\n",
    "    \n",
    "    # qa-pair is positive\n",
    "    # q-another a pair is negative\n",
    "    # q-q pair is negative\n",
    "    # a-a pair is negative\n",
    "    \n",
    "    # Approach\n",
    "    # qa versus a + qa versus q\n",
    "\n",
    "    right_conf = torch.eye(batch_size, device=X.device) * similarities\n",
    "    max_confq, _ = similarities.max(1)\n",
    "    max_confa, _ = similarities.max(0)\n",
    "    total_loss = F.relu(max_confq - right_conf + margin) + F.relu(max_confa - right_conf + margin)    \n",
    "    \n",
    "    return total_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a42F6Nb-yEMc"
   },
   "outputs": [],
   "source": [
    "def bce_loss(X, Y, conf_true=0.9, conf_false=0.1): \n",
    "  '''на вход пришел батч размера n,\n",
    "  мы векторизовали контексты (X)\n",
    "  и ответы (Y) и хотим сделать n*n\n",
    "  независимых классификаций\n",
    "  '''\n",
    "  n = X.shape[0]\n",
    "\n",
    "  logits = torch.mm(X, Y.transpose(0, 1)) # считаем таблицу умножения\n",
    "  identity = torch.eye(n, device=X.device)\n",
    "  \n",
    "  non_diagonal = torch.ones_like(logits) - identity\n",
    "  targets = identity * conf_true + non_diagonal * conf_false\n",
    "  #получаем матрицу с conf_true на диагонали и conf_false где-либо ещё\n",
    "  \n",
    "  weights = identity + non_diagonal / (n - 1)\n",
    "  # ^ чтобы не было перекоса в сторону негативов\n",
    "  return F.binary_cross_entropy_with_logits(logits, targets, weights) * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZP70JppTRF-"
   },
   "outputs": [],
   "source": [
    "def calc_acc(X, Y):\n",
    "    '''на вход пришел батч размера n,\n",
    "    мы векторизовали контексты (X)\n",
    "    и ответы (Y)'''\n",
    "    \n",
    "    csim = cosine_similarity_table(X, Y)\n",
    "    confidence, predictions = csim.max(-1)\n",
    "    avg = confidence.mean().item()\n",
    "    predictions = list(predictions.cpu())\n",
    "    right = 0\n",
    "    for i in range(len(predictions)):\n",
    "        right += predictions[i] == i\n",
    "    return right\n",
    "    \n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))\n",
    "\n",
    "X = torch.tensor([[0, .1], [.1, 0]], device=device)\n",
    "Y = torch.tensor([[.1, 0], [0, .1]], device=device)\n",
    "\n",
    "print(calc_acc(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YiTAp29kXV6-",
    "outputId": "05069111-7709-42a9-af7f-13377c234779",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "  \n",
    "  optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "  ]\n",
    "  \n",
    "  return optimizer_grouped_parameters\n",
    "\n",
    "#get_optimizer_params(qembedder)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(corpus) * .33)\n",
    "train_size = len(corpus) - test_size\n",
    "train_corpus, test_corpus = torch.utils.data.random_split(corpus, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyFunc(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(ApplyFunc, self).__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  batch_size = 10 # 16, 32 are recommended in the paper\n",
    "  trainloader = DataLoader(corpus, batch_size=batch_size, shuffle=True) #\n",
    "  testloader = trainloader # DataLoader(test_corpus, batch_size=batch_size, shuffle=True)\n",
    "  num_train_optimization_steps = len(corpus) * epochs\n",
    "  \n",
    "  lr = 5e-5 # 5e-5, 3e-5, 2e-5 are recommended in the paper\n",
    "  warmup = 0.1\n",
    "  \n",
    "  qoptim = BertAdam(get_optimizer_params(qembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_total=num_train_optimization_steps)\n",
    "  aoptim = BertAdam(get_optimizer_params(aembedder),\n",
    "                                  lr=lr,\n",
    "                                  warmup=warmup,\n",
    "                                  t_toal=num_train_optimization_steps)\n",
    "  criterion = hinge_loss\n",
    "  \n",
    "  '''total = right = 0\n",
    "  bc = BertClient()\n",
    "  for batch in testloader:\n",
    "    total += len(batch[0])\n",
    "    qembeddings = torch.tensor(bc.encode([el.replace('[CLS]', '') for el in batch[0]]))\n",
    "    aembeddings = torch.tensor(bc.encode([el.replace('[CLS]', '') for el in batch[1]]))\n",
    "    right += calc_acc(qembeddings, aembeddings)\n",
    "  print(f'Bert as service {right}/{total}')'''\n",
    "\n",
    "  total = right = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        total += len(batch[0])\n",
    "        embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "        right += calc_acc(*embeddings) \n",
    "\n",
    "  \n",
    "  \n",
    "  logger.info(\"***** Running training *****\")\n",
    "  logger.info(\"  Num steps = %d\", num_train_optimization_steps)  \n",
    "  logger.info(f\"Before training: right: {right} of {total}\")\n",
    "  \n",
    "  qembedder.train()\n",
    "  aembedder.train()\n",
    "  \n",
    "  start_training = time.time()\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    qembedder.train()\n",
    "    aembedder.train()\n",
    "    for bidx, batch in enumerate(tqdm_notebook(iter(trainloader), desc=f\"epoch {epoch}\")):\n",
    "      qoptim.zero_grad()\n",
    "      aoptim.zero_grad()\n",
    "      #print('batch_index', bidx)\n",
    "      embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "      loss = criterion(*embeddings)\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "\n",
    "      qoptim.step()\n",
    "      aoptim.step()\n",
    "    \n",
    "    total = right = 0\n",
    "    qembedder.eval()\n",
    "    aembedder.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            total += len(batch[0])\n",
    "            embeddings = embed_batch(prepare_batch(batch), qembedder, aembedder)\n",
    "            right += calc_acc(*embeddings) \n",
    "\n",
    "    print(f'right: {right} of {total} | loss: {total_loss} ')\n",
    "    \n",
    "  end_training = time.time()\n",
    "  logger.info(f'Training is compleated. Time: {int(end_training - start_training)}')\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_dropout(bert):\n",
    "    drop = nn.Dropout(0.5)\n",
    "    for module in bert.encoder.modules():\n",
    "        if type(module) is type(drop):\n",
    "            module.p = 0\n",
    "            #print(type(module), module)\n",
    "            #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "disable_dropout(qembedder)\n",
    "aembedder = BertModel.from_pretrained(bert_type, cache_dir=cache_dir).to(device)\n",
    "disable_dropout(aembedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "4lPnOyLCLuj_",
    "outputId": "96b3b23e-afce-45e7-efaf-f84c3e5a5fb1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(3) # 2, 3, 4 epochs are recommended in the paper"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unlp_startup.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
